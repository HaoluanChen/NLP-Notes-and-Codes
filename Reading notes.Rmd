---
title: "Reading course notes"
author: "Haoluan Chen"
date: "9/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(ggplot2)
library(tidyr)
library(scales)
library(wordcloud)

```

# Chapter 1 
## 1 The tidy text format
Tidy data structure: 

* Each variable is a column
* Each observation is a row
* Each type of observational unit is a table

Some defination:\
Tidy text format - a table with one-token-per-row\
Token - a meaningful unit of text (e.g a word that we are interested in analysis)\
Tokenization - a process of splitting text into tokens\

## 1.1 Contrasting tidy text with other data structures
Other text data structures: 

* String: character vectors
* Corpus: contain raw strings annotated with additional metadata and details
* Document-term matrix: a sparse matrix describing a collection of documents with one row for each document and one column for each term. (e.g word count and tf-idf)

## 1.2 The unnest_tokens function
https://toronto.ctvnews.ca/ontario-s-covid-19-case-count-surges-to-highest-level-since-june-1.5100868

This is a news updating the current COVID-19 case count in Ontario

```{r}
text <- c("TORONTO -- Ontario’s daily COVID-19 case count has surpassed 200 for the first time in almost two months." ,"On Friday, provincial health officials logged 213 new patients infected with the novel coronavirus.")

text
```

This text is the first two paragraph in the article and we want to turn it into a tidy test dataset to do further analyze. First, we need to put it into a data frame

```{r}
text_df <- tibble(line = 1:2, text = text)

text_df
```

Note that this output is a tibble data fram, it has a convenient print method, will not convert strings to factors, and does not user row names.

Now, we can convert this data frame into tidy format using unnest_tokens() function

```{r}
text_df %>%
  unnest_tokens(word, text)
```

unnest_tokens() takes in two arguments, the first one is the output column name (word in this case) and the second one is the input column (takes the text column in the text_df data frame)

From the output, we notice that:

* the line column represents the line number each word came from
* punctuation has been stripped
* converted to lower case (we can use to_lower = FALSE argument to turn off this behavior)

## 1.3 Tidying the works of Jane Austen (news in my case)

```{r}
news <- c("TORONTO -- Ontario’s daily COVID-19 case count has surpassed 200 for the first time in almost two months." ,"On Friday, provincial health officials logged 213 new patients infected with the novel coronavirus." , "The other 26 local public health units in Ontario reported five or fewer COVID-19 cases on Friday, with 18 reporting none at all.", "The last time Ontario saw a daily case count climb above 200 was on July 21 and the last time the number of infections recorded in a single day was higher than 213 was on June 23 when 216 cases were confirmed.","The province’s daily count had hovered above the 100 mark for majority of the past three weeks.", "Most recently, Ontario saw 170 new cases of the disease confirmed on Thursday, 149 on Wednesday, 185 on Tuesday and 190 on Monday. The last time the province dipped into double digits was on Aug. 26 and before that it was on Aug. 20.", "The new patients logged on Friday bring Ontario’s total case count to 44,068, including deaths and recoveries.", "There were no new deaths linked to the novel coronavirus recorded by the province on Friday, but health officials did retract one deceased patient from Ontario’s death toll, which is now 2,813." , "Health officials deemed 124 more COVID-19 cases to be resolved in the province as of Friday. Ontario’s total number of recovered patients is now 39,598.", "There are now 1,657 active cases in the province.", "As of Friday, 49 COVID-19 patients are in Ontario hospitals. Eighteen of those patients are in the intensive care unit and nine of those patients are breathing with the assistance of a ventilator.", "Since the start of the pandemic, more than 3.2 million COVID-19 tests have been conducted in Ontario.", "In the last recorded 24-hour period, 32,501 tests were conducted.", "On July 21, nearly 23,000 COVID-19 tests were conducted and just more than 16,000 were conducted on June 23.", "There are currently 31,384 tests under investigation in the province.")
```

```{r}
news
news_df <- tibble(line = 1:15, text = news)

news_df <- news_df %>%
  unnest_tokens(word, text)
```

Now we have tokenized the news, we can manipulate it with tidy tools. Often we want to revmoe stop words which are not userful for an analysis. We may remove the stop words by using anti_join

```{r}
news_df <- anti_join(news_df, stop_words, by = "word")
```

Then, we can use count() to find the most common words in this news article

```{r}
news_df %>% count(word, sort = TRUE)
```

Plot to show the most common words

```{r}
plot1 <- news_df %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 3) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word,n)) + 
  geom_col() +
  xlab(NULL) + 
  coord_flip()
```

Here is the most common words in the news article

## 1.4 The gutenbergr package
a package privides access to the data set used in the book. For me I will use another new article related to COVID-19. 
https://www.cp24.com/news/ontario-reports-decline-in-new-covid-19-cases-today-sixth-day-in-a-row-new-infections-remain-under-100-1.5056474?cache=yes%3FclipId%3D89531

## 1.5 Word frequencies
A common task in text mining is to look at word frequencies. In this section, I will compare the two news article above. The first article is a recent one, which the COVID cases is increasing. The second article in 1.4 was a news article in August 8, which the COVID case is decreasing.

```{r}
news2 <- c("Ontario is reporting a decline in new COVID-19 cases today, marking the sixth day in a row new infections remained under 100.

Provincial health officials say 70 new cases of the virus were confirmed today, down from the 88 reported one day prior.

Officials recorded 95 new cases on Thursday, 86 on Wednesday, and 91 on Tuesday, bringing the rolling five-day average of new cases to 86, down from 107 one week ago.
Ontario Health Minister Christine Elliott confirmed Saturday that recoveries outpaced new cases once again today.

According to the province's latest  epidemiological summary, 107 more cases are now considered to be resolved.

More than 26,000 tests were completed over the past 24 hours, resulting in a case positivity rate of 0.27 per cent.

'Locally, 29 of Ontario’s 34 public health units are reporting five or fewer cases with 15 of them reporting no new cases,' Elliott wrote in a tweet published on Saturday morning.

Ottawa and Peel Region each reported 13 new infections and nine new cases were confirmed in Chatham-Kent.

Toronto recorded the fewest number of new infections in the GTA with just one new case confirmed over the past 24 hours.

Nearly 60 per cent of all new cases in the province were in people under the age of 40.

Elliott noted that hospitalizations and intensive care admissions both declined.

The latest data indicates that 53 people infected with COVID-19 are currently receiving treatment at Ontario hospitals. Of those patients, 27 are in intensive care and 12 remain on ventilators.

One new death was reported today, bringing the total number of virus-related deaths in the province to 2,784.

The total number of lab-confirmed cases of COVID-19 in Ontario now stands at 39,967 but only 1,052 are still considered to be active.")

news2
news2_df <- tibble(line = 1, text = news2)

news2_df <- news2_df %>%
  unnest_tokens(word, text)

news2_df <- news2_df %>% 
  anti_join(stop_words)

news2_df %>% count(word, sort = TRUE)

news2_df %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 2) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word,n)) + 
  geom_col() +
  xlab(NULL) + 
  coord_flip()
plot1
```

Lets compare the most common word between two article.

Note: I had some problem producing the plot in this seesion
in comparing frequency, So I am comparing the most common words between two article.
```{r eval=FALSE, include=FALSE}
frequency <- bind_rows(mutate(news_df, news = "1"),
                       mutate(news2_df, news = "2") ) %>% 
  count(news, word) %>% 
  group_by(news) %>% 
  mutate(proportion = n/sum(n)) %>% 
  select(-n) %>% 
  spread(news, proportion) %>% 
  gather(news, proportion,2:3)

frequency

```

## 1.6 Summary
In this chapter, I learning that when data is organized into tidy text data(one token per row), tasks like removing stop words and ploting are simple within the tidy tool ecosystem. The one-token-per-row framework may be extended from words to n-grams and other meaningful unit of text.

## 2 Sentiment analysis with tidy data
We can use the tools of text mining to approach the emotional conents of text programmatically. One way to analyze the sentiment of a text is to consider the text as a combinations of individual words and the sentiment content of the text is the sum of the sentiment content of each words. 

## 2.1 The sentiments dataset
The following three package provides access to several sentiment lexicons(the vocabulary of a person, language, or branch of knowledge): 

* AFINN (from AFINN from Nielsen)
* bing (from Bing Liu and collaborators)
* nrc (from Saif Mohammad and Peter Turney)

All these lexicons are based on unigrams(single words). 


In the AFINN package, the lexicon assigns each words with a score between -5 to 5, where negatives socres indicating negative sentiment and postive socres means the postive sentiment. 

In the bing package, the lexicon also categorizes words in binary fashion but only postive and negative categories.

In the nrc package, the lexicon categorizes words into categories(e.g postive, negative, anger, disgust, fear, sadness) in a binary fashion. 

Note: cite them when using these sentiment lexicon, 
```{r}
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```
These sentiment lexicons were constructed and validated by crowdsourcing, restauant or movie reviews, or Twitter data. We should be careful when applying these sentiment lexicons on the text that is very different from what they were validated on. There are also some domain-specific sentiment lexicons, which are constructed to be used with text from a specific content area.(e.g some sentiment lexicons are specifically for finance)

Something to keep in mind:

* Not every English words is in the lexicon, because many words are neutral. 
* these methods do not take account qualifers before a word. For example, "no good" or "not true". 
* The size of the text can have an effect on our analysis result. A text with many paragraphs can have both postive and negative sentiment and averaged out to be close to zero. Thus, sentence-sized or paragraph-sized text often works better. 

## 2.2 Sentiment analysis with inner join
With tidy text data format, we can perform sentiment analysis by using inner join. 

Using NRC lexicon, What are the most common sad words in our first article?
(COVID-19 cases increasing)

```{r}

nrc_negative <- get_sentiments("nrc") %>% 
  filter(sentiment == "negative")

news_df %>% 
  inner_join(nrc_negative) %>% 
  count(word, sort = TRUE)
```

Since we have a small data set, there are only four negative words in our first article.

```{r}
news2_df %>% 
  inner_join(nrc_negative) %>% 
  count(word, sort = TRUE)
```

These are the negative words in the second article. 

We can also find the sentiment score for each word in the article using Boing lexicon and inner_join

```{r}
news_sentiment <- news_df %>% 
  inner_join(get_sentiments("bing")) 

news_sentiment <- news_df %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(index = line, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)


news_sentiment
```
```{r}
ggplot(news_sentiment, aes(index, sentiment)) +
  geom_col(show.legend = FALSE)+
  scale_x_discrete(limits = c(0, 2, 4, 6, 8))
```

As the result shown, our article are likely to be negative. (Again we have a very small dataset)

## 2.3 Comparing the three sentiment dictionaries
Let's use all three sentiment lexicons on the article.

```{r}
afinn <- news_df %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = line) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

afinn

bing <- news_df %>% 
  inner_join(get_sentiments("bing")) %>%
  mutate(method = "bing")
bing

nrc <- news_df %>% 
  inner_join(get_sentiments("nrc")) %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  mutate(method = "NRC")
nrc

bing_and_nrc <- bind_rows(bing, nrc) %>% 
  count(method, index = line, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)
bing_and_nrc

# plotting
bind_rows(afinn, bing_and_nrc) %>% 
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col() +
  facet_wrap(~method, ncol = 1, scales = "free_y") + 
  scale_x_discrete(limits = c(0, 2, 4, 6, 8, 10, 12))
  
```

As we can see in the plot, three different lexicons for calculating sentiment in our article gives very different result due to the small data size. 

However, according to the book, we see similar dips and peaks in sentiment at about the same places in the novel, but absolute values are significantly different. AFINN lexicon gives largest absolute value, Bing gives lower absolute values, and the NRC labled the text more positively.

"the NRC sentiment is high, the AFINN sentiment has more variance, the Bing et al. sentiment appears to find longer stretches of similar text, but all three agree roughly on the overall trends in the sentiment through a narrative arc."

Why is NRC lexicon provided higher result compared to the Bing?
```{r}
get_sentiments("nrc") %>% 
     filter(sentiment %in% c("positive", "negative")) %>% 
count(sentiment)

get_sentiments("bing") %>% 
  count(sentiment)
```

As we can see, both lexicons ahve more negative words than postive words, but the ratio of negative to postive words is higher in the Bing than NRC. This is the reason that we see higher sentiment scores in the NRC.

Also, if the negative words in the NRC do not match the words we are analysing very well, then "whatever the source of these differences, we see similar relative trajectories across the narrative arc, with similar changes in slope, but marked differences in absolute sentiment from lexicon to lexicon."

This is important to keep in mind when choosing a sentiment lexicon for analysis. 

## 2.4 Most common positive and negative words
```{r}
nrc_word_count <- nrc %>% 
  count(word, sentiment, sort = TRUE)

nrc_word_count

#plot

nrc_word_count %>% 
  group_by(sentiment) %>% 
  ggplot(aes(word,n,fill = sentiment)) +
  geom_col(show.legend = FALSE) + 
   facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment", x = NULL) +
  coord_flip()
```

Problem: Since my article data set is small, we can spot problem in my analysis that patient and confirmed are negative in my case. How do we spot there kinds of problem when we have very big dataset?

## 2.5 Wordclouds 
```{r}
library(janeaustenr)
library(dplyr)
library(stringr)

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",                             ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

library(reshape2)
tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

Note that since my data set is small, I am using the data set in the text book to create these wordcloud.

To tag positive and negative words, we will have to turn the datafram into matrix with reshape2 (acast()) function and use comparison.cloud().

The size of the word's test in the wordcloud is in proportion to its frequency in the document. We use word cloud to visually see the most frequent postive and negative words, but the size is not comparable across sentiments

## 2.6 Looking at units beyond just words
Sometimes it is useful to look at different unit of text. For example, try to understand the sentiment of a sentence as a whole. We can tokenize text into sentences.
```{r}
news_sentences <- tibble(line = 1:15, text = news)

news_sentences <- news_sentences %>%
  unnest_tokens(sentence, text, token = "sentences")
news_sentences
```

We can use unnest_tokens() to split into tokens using a regex patter. It is not needed for my case.

Now, we can find the porportion of negative words in the article. 

```{r}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
word_count <- nrow(news_df)


news_df %>%
  semi_join(bingnegative)%>%
  summarize(negativewords = n()) %>%
  mutate(ratio = negativewords/word_count)

```

We can compare the negative wor ration with other documents, since it is normalized by the text length. 

## 2.7 Summary
Sentiment analysis can help understand the attitudes and opnions expressed in text. "We can use sentiment analysis to understand how a narrative arc changes throughout a book or what words with emotional and opinon content are important for a particular text."
We can also compare differnt emotional and opinion content between different documents. 