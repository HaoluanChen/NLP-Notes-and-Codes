---
title: "NLP intermediate ch 3-5"
author: "Haoluan Chen"
date: "11/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
library(hcandersenr)
library(tidyverse)
library(tidytext)
library(SnowballC)
```


## Chapter 3 Stop words
Not every words carry the same amount of information. Common words that carry little (or no) meaningful information are called stop words. It is common advice and practice to remove stop words for various NLP tasks.

"Historically, one of the main reasons for removing stop words was to decrease the computational time for text mining; it can be regarded as a dimensionality reduction of text data and was commonly used in search engines to give better results"

We cenerally categorize stop words into three groups: global, subject and document stop words.

* Global stop words - almost always low in meaning ("of", "and") likely a safe bet for removal.
* subject-specific stop words - Words are uninformative for a given subject. It have to construct the stop word list manually because it is case by case.
* document level stop words - Words do not provide any or much infomation for a given document. (hard to classify and identify) the impact of these words on the regression or classification tasks are not obvious.

## 3.1 Using premade stop word lists
A quick option for using stop words is to get a list that has already been created. However, we will have to be careful of the typos, inclusion of clearly informative words , and internal inconsistencies. 

We should always inspect and verify the list we are using as well as check that it is appropriate for our use case.  

For this book, we mainly focus on three of the list of English stop words provided by the stopwords package

Look into the content inside the lists

```{r}
library(stopwords)
length(stopwords(source = "smart"))
length(stopwords(source = "snowball"))
length(stopwords(source = "stopwords-iso"))
```

The length of these stop word lists are quite different 

```{r}
str_subset(stopwords(source = "smart"), "'")
```

This SMART list includes "he's" but not "she's". This chould be worth rectifying before applying these stop word list. This stop word list was likely generated by selecting the most frequent words across a large corpus that had bias towards to men. Once again, we should always look carefully at any premade word list or another artifact we use, and make sure it works well with our needs.

As size of the stop word list grows, each word added will have a diminishing positive effect with the increasing risk that a meaningful word has been placed on the list by mistake.

## 3.1.1 Stop word removal in R
Now we have the stop word list, we can use filter() from dplyr package with a negated "%in%" if you have the stop words as vector or use anti_join if stop words are in a tibble().

```{r}
fir_tree <- hca_fairytales() %>%
  filter(
    book == "The fir tree",
    language == "English"
  )

tidy_fir_tree <- fir_tree %>%
  unnest_tokens(word, text)

tidy_fir_tree %>%
  filter(!(word %in% stopwords(source = "snowball")))
```
If we use get_stopwords() from tidytext, we can use anti_join() to remove stop words
```{r}
tidy_fir_tree %>%
  anti_join(get_stopwords(source = "snowball"))
```

## 3.2 Creating your own stop words list
One way to create our own stop words list is to find the most frequent words in a large text data and go over select the top most frequent words. Select the number of words to remove is best done by a case-by-case basis. We can start with a lower number like twenty and increase by ten words until you get to words that are not appropriate as stop wrods for your analytical purpose.

This list may not be perfect. Depending on how text was generated or processed, some words may be added to stop word list due to encoding, and optical character recognition errors. Further, these result may be biased because they are based on the corpus of documents we have aviable.

Note: This bias can be minimized by removing words we would expect to be over-represented or to add words we expect to be under-represented.

By using a single threshold to create a stop word list, you would likely only include one form of words like he-she, his-her, man-woman. 

We can also calculate the IDF from a big text data. Then look the words with lowest IDF, and select the stop words. 

## 3.3 All stop word lists are context-specific

Context is important in text modeling. One common concern to consider is how pronouns bring information to your text. Pronouns are included in many different stop word lists (although inconsistently) but they will often not be noise in text data. Sometimes you will have to add in words yourself, depending on the domain. 

## 3.4 What happens when you remove stop words

The larger stop word lists remove more words than shorter stop word lists. And shorter texts have a lower percentage of stop words.

Another problem you may face is dealing with misspellings. Handling misspellings when using pre-made lists can be done by manually adding common misspellings.

One of the downsides of creating your own stop word lists using frequencies is that you are limited to using words that you have already observed.

Given the right list of words, we see no harm to the model performance, and may even find improvement in due to noise reduction 

## 3.5 Stop words in languages other than English
You should expect different languages to have a different number of “uninformative” words. And other languages may be more complex to identify the stop words in its context. 

## Chapter 4 Stemming

When we deal with text, often documents contains different versions of one base word, which are called a stem.

for example in the book "The fir Tree", both tree and trees appears in the book, but we are not really interested in the difference between trees and tree. So we want to treat both together. 

Stemming is the process of identifying the base word (stem) for a data set of words. Stemming is concerned with the linguistics sub-field of morphology. 

## 4.1 How to stem text in R
We can use SnowballC package in R to stem text. 
```{r}
tidy_fir_tree <- fir_tree %>%
  unnest_tokens(word, text) %>%
  anti_join(get_stopwords())

tidy_fir_tree %>%
  mutate(stem = wordStem(word)) %>%
  count(stem, sort = TRUE)
```
Note that the algorithm identifies the stem of both story and stories as stori, not a regular English word but a special stem object. 

Another stemmer is the stemming algorithm of the Hunspell library. 
```{r}
library(hunspell)

tidy_fir_tree %>%
  mutate(stem = hunspell_stem(word)) %>%
  unnest(stem) %>%
  count(stem, sort = TRUE)
```
With this stemmer, we have only real English words and more total rows in the result. It is because we have two stems.
```{r}
hunspell_stem("discontented")
```

This stemmer works differently; it uses both morphological analysis of a word and existing dictionaries to find possible stems. It’s possible to end up with more than one, and it’s possible for a stem to be a word that is not related by meaning to the original word. For example, one of the stems of “number” is “numb” with this library.

Since Hunspell library was built to be a spell checker, so it may not be appropriate choice for some purposes. 

## 4.2 Should you use stemming at all?
We should think of stemming as a pre-processing step in text modeling, one that must be thought through and chosen with good judgment.

Why does stemming often help? It reduces the feature space of text data.

```{r}
tidy_fir_tree %>% 
  count(word, sort = TRUE)
```

There are 639 distinct words in the book. 

A common data structure for modeling is a matrix. We can use cast() to create a sparse matrix 

```{r}
hca_fairytales() %>% filter(language == "English") %>% 
  unnest_tokens(word, text) %>% 
  count(book, word) %>% 
  anti_join(get_stopwords()) %>% 
  cast_dfm(book, word, n)
```
```{r}
hca_fairytales() %>% filter(language == "English") %>% 
  unnest_tokens(word, text) %>% 
  count(book, word) %>% 
  anti_join(get_stopwords()) %>% 
  mutate(stem = wordStem(word)) %>% 
  count(book, stem) %>% 
  cast_dfm(book, stem, n)
```
As we can see, after stemming, we reduced the number of word features by many thousands, although the sparsity did not change much. So assuming we have not lost any important information by stemming, reducing the number of word feature so dramatically will improve the performance of any machine learning model. 

Note that there is growing body of academic research demonstrating that stemming can be counterproductive for text modeling. 

"Despite their frequent use in topic modeling, we find that stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability." (Schofield and Mimno, 2016)

In most cases stemming algorithms will successfully reduce words to stem, but in some cases, it will collapsing some words with dramatic different in meaning or semantics. For example, meaning and mean, university and universe.

temming can increase a model’s ability to find the positive examples. However, if the complaint text is over-stemmed, the resulting model loses its ability to label the negative examples

## 4.3 Understand a stemming algorithm
Refer back to the book :)

## 4.4 Handling punctuation when stemming
Punctuation is typically less information-dense than the words themselves and thus it is often removed early in a text mining analysis project, but it is important to thinking through the impact of punctuation specifically on stemming. 

```{r}
tidy_fir_tree %>%
  count(word, sort = TRUE) %>%
  filter(str_detect(word, "^tree"))
```

We can see that tree's is belong ti the same stem as "trees" and "tree"

We can use regular expression to split the tokens not only on white space but also on punctuation.
```{r}
fir_tree_counts <- fir_tree %>%
  unnest_tokens(word, text, token = "regex", pattern = "\\s+|[[:punct:]]+") %>% # space or punctuation
  anti_join(get_stopwords()) %>%
  mutate(stem = wordStem(word)) %>%
  count(stem, sort = TRUE)

fir_tree_counts %>%
  filter(str_detect(stem, "^tree"))
```
Now, we have put all the related words together and identified them with the same stem.

## 4.6 Lemmatization and stemming
Stemming is typically implemented in NLP as rule-based, operating on the word by itself. Lemmatization is another way to normalizing words to have a root. Lemmatization uses knowledge about a language's structure to reduce words down to their lemmas. the canonical or dictionary form of words. It is typically implemented in NLP as linguistics-based, operating on the word in its context. 

A modern, efficient implementation for lemmatization is available in the excellent spacy library

Implementing lemmatization is slower and more complex than stemming. Just like with stemming, lemmatization often improves the true positive rate (or recall) but at the expense of the true negative rate (or precision) compared to not using lemmatization, but typically less so than stemming.

## 4.7 Stemming and stop words
Stop word lists are usually unstemmed, so you need to remove stop words before stemming text data. 

## Chapter 5 Word Embeddings
Linguists have long worked on vector models for language that can reduce the number of dimensions representing text data based on how people use language. These kinds of dense word vectors are often called word embedding.

## 5.1 Understand word embeddings by finding them yourself
Word embedding are a way to represent text data as vectors of numbers based on a huge corpus of text, capturing semantic meaning from words’ context.

Modern word embeddings are based on a statistical approach to modeling language, rather than a linguistics or rules-based approach.

We can determine these vectors for a corpus of text using word counts and matrix factorization. This allows us to find word vectors for their own collections of text. 

Now we can create a slide_windows() function using slide() function from the slider package

Our new function identifies skipgram windows to calculate the skipgram probabilities, which is how often we find each word near each other word. We do this by defining a fixed-size moving window that centers around each word. Do we see word1 and word2 together within this window? We can calculate probabilities based on when we do or do not.

window_size is a arguments for our function, A smaller window size, like three or four, focuses on how the word is used and learns what other words are functionally similar. A larger window size, like ten, captures more information about the domain or topic of each word, not constrained by how functionally similar the words are.

```{r}
slide_windows <- function(tbl, window_size) {
  skipgrams <- slider::slide(
    tbl,
    ~.x,
    .after = window_size - 1,
    .step = 1,
    .complete = TRUE
  )

  safe_mutate <- safely(mutate)

  out <- map2(
    skipgrams,
    1:length(skipgrams),
    ~ safe_mutate(.x, window_id = .y)
  )

  out %>%
    transpose() %>%
    pluck("result") %>%
    compact() %>%
    bind_rows()
}
```

```{r}
complaints <- read_csv("complaints.csv")
tidy_complaints <- complaints %>%
  select(`Complaint ID`, `Consumer complaint narrative`) %>%
  unnest_tokens(word, `Consumer complaint narrative`) %>%
  add_count(word) %>%
  filter(n >= 50) %>%
  select(-n)

nested_words <- tidy_complaints %>%
  nest(words = c(word))

nested_words
```


```{r eval=FALSE, include=FALSE}
library(widyr)
library(furrr)

plan(multiprocess) ## for parallel processing

tidy_pmi <- nested_words %>%
  mutate(words = future_map(words, slide_windows, 4,
    .progress = TRUE
  )) %>%
  unnest(words) %>%
  unite(window_id, complaint_id, window_id) %>%
  pairwise_pmi(word, window_id)

tidy_pmi
```
I am not sure why the code in the text book does not work even when I copy and pasted them... I will move on to pre-trained word embeddings

## 5.3 Use pre-trained word embeddings
If the data set is too small, you usually cannot train reliable word embeddings. Thus, we can use pre-trained word embeddins such as the GloVe word vectors trained on six billion tokens. It is available in the textdata package. 

```{r}
library(textdata)

glove6b <- embedding_glove6b(dimensions = 50)
glove6b
```
We can transform these word embeddings into a more tidy format, using pivot_longer() from tidyr. 

```{r}
tidy_glove <- glove6b %>%
  pivot_longer(contains("d"),
    names_to = "dimension"
  ) %>%
  rename(item1 = token)

tidy_glove
```
Explore the synonyms in the embedding space.

```{r}
library(widyr)
nearest_neighbors <- function(df, token) {
  df %>%
    widely(
      ~ . %*% (.[token, ]),
      sort = TRUE,
      maximum_size = NULL
    )(item1, dimension, value) %>%
    select(-item2)
}

tidy_glove %>%
  nearest_neighbors("error")
```
What is closest to the word "month" in these pre-trained GloVe embeddings?
```{r}
tidy_glove %>%
  nearest_neighbors("statistics")
```
To integrate the pre-trained word embeddings, we can use inner_join() to match the tokens in pre-trained data set and our data set. 

We may lost some documents which contains words are not included in the GloVe embeddings. 

## 5.4 Fairness and word embeddings
The embedding are trained or learned from a large corpus of text data. So, the prejudice and bias exists in the corpus will also reflects on the embedding.

This is true of all machine learning to some extent. Here is some GloVe word embeddings replicate human-like semantic biases:

* Typically Black first names are associated with more unpleasant feelings than typically white first names.
* Women’s first names are more associated with family and men’s first names are more associated with career.
* Terms associated with women are more associated with the arts and terms associated with men are more associated with science.

When embeddings with these kinds of stereotypes are used as a pre-processing step in training a predictive model, the final model can exhibit racist, sexist, or otherwise biased characteristics. 

## 5.5 Using word embeddings in the real world
Depending on the particular analytical question you are trying to answer, another numerical representation of text data (such as word frequencies or tf-idf of single words or n-grams) may be more appropriate. 

Building your own vectors is likely to be a good option when the text domain you are working in is specific rather than general purpose; some examples of such domains could include customer feedback for a clothing e-commerce site, comments posted on a coding Q&A site, or legal documents. However, learning good quality word embeddigns is only realistic when you have large corpus of text data. 

NLP researchers have also proposed methods for debiasing embeddings. For example choosing specific sets of words that are re-projected in the vector space so that some bias is mitigated. However, some researcher showed that bias may still exists after applying current debiasing methods. 

## Summary and Reflection

