---
title: "Reading course notes"
author: "Haoluan Chen"
date: "9/11/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(ggplot2)
library(scales)
library(tidyr)
library(wordcloud)
library(igraph)
```

# Chapter 1 
## 1 The tidy text format
Tidy data structure: 

* Each variable is a column
* Each observation is a row
* Each type of observational unit is a table

Some definition:\
Tidy text format - a table with one-token-per-row\
Token - a meaningful unit of text (e.g a word that we are interested in analysis)\
Tokenization - a process of splitting text into tokens\

## 1.1 Contrasting tidy text with other data structures
Other text data structures: 

* String: character vectors
* Corpus: contain raw strings annotated with additional metadata and details
* Document-term matrix: a sparse matrix describing a collection of documents with one row for each document and one column for each term. (e.g word count and tf-idf)

## 1.2 The unnest_tokens function
https://toronto.ctvnews.ca/ontario-s-covid-19-case-count-surges-to-highest-level-since-june-1.5100868

This is a news updating the current COVID-19 case count in Ontario

```{r}
text <- c("TORONTO -- Ontario’s daily COVID-19 case count has surpassed 200 for the first time in almost two months." ,"On Friday, provincial health officials logged 213 new patients infected with the novel coronavirus.")

text
```

This text is the first two paragraph in the article and we want to turn it into a tidy test dataset to do further analyze. First, we need to put it into a data frame

```{r}
text_df <- tibble(line = 1:2, text = text)

text_df
```

Note that this output is a tibble data fram, it has a convenient print method, will not convert strings to factors, and does not user row names.

Now, we can convert this data frame into tidy format using unnest_tokens() function

```{r}
text_df %>%
  unnest_tokens(word, text)
```

unnest_tokens() takes in two arguments, the first one is the output column name (word in this case) and the second one is the input column (takes the text column in the text_df data frame)

From the output, we notice that:

* the line column represents the line number each word came from
* punctuation has been stripped
* converted to lower case (we can use to_lower = FALSE argument to turn off this behavior)

## 1.3 Tidying the works of Jane Austen (news in my case)

```{r}
news <- c("TORONTO -- Ontario’s daily COVID-19 case count has surpassed 200 for the first time in almost two months." ,"On Friday, provincial health officials logged 213 new patients infected with the novel coronavirus." , "The other 26 local public health units in Ontario reported five or fewer COVID-19 cases on Friday, with 18 reporting none at all.", "The last time Ontario saw a daily case count climb above 200 was on July 21 and the last time the number of infections recorded in a single day was higher than 213 was on June 23 when 216 cases were confirmed.","The province’s daily count had hovered above the 100 mark for majority of the past three weeks.", "Most recently, Ontario saw 170 new cases of the disease confirmed on Thursday, 149 on Wednesday, 185 on Tuesday and 190 on Monday. The last time the province dipped into double digits was on Aug. 26 and before that it was on Aug. 20.", "The new patients logged on Friday bring Ontario’s total case count to 44,068, including deaths and recoveries.", "There were no new deaths linked to the novel coronavirus recorded by the province on Friday, but health officials did retract one deceased patient from Ontario’s death toll, which is now 2,813." , "Health officials deemed 124 more COVID-19 cases to be resolved in the province as of Friday. Ontario’s total number of recovered patients is now 39,598.", "There are now 1,657 active cases in the province.", "As of Friday, 49 COVID-19 patients are in Ontario hospitals. Eighteen of those patients are in the intensive care unit and nine of those patients are breathing with the assistance of a ventilator.", "Since the start of the pandemic, more than 3.2 million COVID-19 tests have been conducted in Ontario.", "In the last recorded 24-hour period, 32,501 tests were conducted.", "On July 21, nearly 23,000 COVID-19 tests were conducted and just more than 16,000 were conducted on June 23.", "There are currently 31,384 tests under investigation in the province.")
```

```{r}
news_df <- tibble(line = 1:15, text = news)
news_token <- news_df %>% unnest_tokens(word, text) %>% 
  count(word, sort =TRUE)
news_token
```

Now we have tokenized the news, we can manipulate it with tidy tools. Often we want to revmoe stop words which are not userful for an analysis. We may remove the stop words by using anti_join

```{r}
news_token <- anti_join(news_token, stop_words, by = "word")
news_token
```

Then, we can use count() to find the most common words in this news article

```{r}
news_df %>% count(word, sort = TRUE)
```

Plot to show the most common words

```{r}
plot1 <- news_token %>% 
  filter(n > 3) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word,n, fill = "red")) + 
  geom_col() +
  xlab(NULL) + 
  coord_flip()
plot1
```

Here is the most common words in the news article

## 1.4 The gutenbergr package
a package privides access to the data set used in the book. For me I will use another new article related to COVID-19. 
https://www.cp24.com/news/ontario-reports-decline-in-new-covid-19-cases-today-sixth-day-in-a-row-new-infections-remain-under-100-1.5056474?cache=yes%3FclipId%3D89531

## 1.5 Word frequencies
A common task in text mining is to look at word frequencies. In this section, I will compare the two news article above. The first article is a recent one, which the COVID cases is increasing. The second article in 1.4 was a news article in August 8, which the COVID case is decreasing.

```{r}
news2 <- c("Ontario is reporting a decline in new COVID-19 cases today, marking the sixth day in a row new infections remained under 100.

Provincial health officials say 70 new cases of the virus were confirmed today, down from the 88 reported one day prior.

Officials recorded 95 new cases on Thursday, 86 on Wednesday, and 91 on Tuesday, bringing the rolling five-day average of new cases to 86, down from 107 one week ago.
Ontario Health Minister Christine Elliott confirmed Saturday that recoveries outpaced new cases once again today.

According to the province's latest  epidemiological summary, 107 more cases are now considered to be resolved.

More than 26,000 tests were completed over the past 24 hours, resulting in a case positivity rate of 0.27 per cent.

'Locally, 29 of Ontario’s 34 public health units are reporting five or fewer cases with 15 of them reporting no new cases,' Elliott wrote in a tweet published on Saturday morning.

Ottawa and Peel Region each reported 13 new infections and nine new cases were confirmed in Chatham-Kent.

Toronto recorded the fewest number of new infections in the GTA with just one new case confirmed over the past 24 hours.

Nearly 60 per cent of all new cases in the province were in people under the age of 40.

Elliott noted that hospitalizations and intensive care admissions both declined.

The latest data indicates that 53 people infected with COVID-19 are currently receiving treatment at Ontario hospitals. Of those patients, 27 are in intensive care and 12 remain on ventilators.

One new death was reported today, bringing the total number of virus-related deaths in the province to 2,784.

The total number of lab-confirmed cases of COVID-19 in Ontario now stands at 39,967 but only 1,052 are still considered to be active.")

news2
news2_df <- tibble(line = 1, text = news2)

news2_df <- news2_df %>%
  unnest_tokens(word, text)

news2_df <- news2_df %>% 
  anti_join(stop_words)

news2_df %>% count(word, sort = TRUE)

news2_df %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 2) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word,n)) + 
  geom_col() +
  xlab(NULL) + 
  coord_flip()
plot1
```

Lets compare the most common word between two article.

Note: I had some problem producing the plot in this session
in comparing frequency, So I am comparing the most common words between two article.
```{r eval=FALSE, include=FALSE}
frequency <- bind_rows(mutate(news_df, news = "1"),
                       mutate(news2_df, news = "2") ) %>% 
  count(news, word) %>% 
  group_by(news) %>% 
  mutate(proportion = n/sum(n)) %>% 
  select(-n) %>% 
  spread(news, proportion) %>% 
  gather(news, proportion,2:3)

frequency

```

## 1.6 Summary
In this chapter, I learning that when data is organized into tidy text data(one token per row), tasks like removing stop words and ploting are simple within the tidy tool ecosystem. The one-token-per-row framework may be extended from words to n-grams and other meaningful unit of text.

## 2 Sentiment analysis with tidy data
We can use the tools of text mining to approach the emotional conents of text programmatically. One way to analyze the sentiment of a text is to consider the text as a combinations of individual words and the sentiment content of the text is the sum of the sentiment content of each words. 

## 2.1 The sentiments dataset
The following three package provides access to several sentiment lexicons(the vocabulary of a person, language, or branch of knowledge): 

* AFINN (from AFINN from Nielsen)
* bing (from Bing Liu and collaborators)
* nrc (from Saif Mohammad and Peter Turney)

All these lexicons are based on unigrams(single words). 


In the AFINN package, the lexicon assigns each words with a score between -5 to 5, where negatives socres indicating negative sentiment and postive socres means the postive sentiment. 

In the bing package, the lexicon also categorizes words in binary fashion but only postive and negative categories.

In the nrc package, the lexicon categorizes words into categories(e.g postive, negative, anger, disgust, fear, sadness) in a binary fashion. 

Note: cite them when using these sentiment lexicon, 
```{r}
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```
These sentiment lexicons were constructed and validated by crowdsourcing, restauant or movie reviews, or Twitter data. We should be careful when applying these sentiment lexicons on the text that is very different from what they were validated on. There are also some domain-specific sentiment lexicons, which are constructed to be used with text from a specific content area.(e.g some sentiment lexicons are specifically for finance)

Something to keep in mind:

* Not every English words is in the lexicon, because many words are neutral. 
* these methods do not take account qualifers before a word. For example, "no good" or "not true". 
* The size of the text can have an effect on our analysis result. A text with many paragraphs can have both postive and negative sentiment and averaged out to be close to zero. Thus, sentence-sized or paragraph-sized text often works better. 

## 2.2 Sentiment analysis with inner join
With tidy text data format, we can perform sentiment analysis by using inner join. 

Using NRC lexicon, What are the most common sad words in our first article?
(COVID-19 cases increasing)

```{r}

nrc_negative <- get_sentiments("nrc") %>% 
  filter(sentiment == "negative")

news_df %>% 
  inner_join(nrc_negative) %>% 
  count(word, sort = TRUE)
```

Since we have a small data set, there are only 5 negative words in our first article.

```{r}
news2_df %>% 
  inner_join(nrc_negative) %>% 
  count(word, sort = TRUE)
```

These are the negative words in the second article. 

We can also find the sentiment score for each word in the article using Boing lexicon and inner_join

```{r}
news_sentiment <- news_df %>% 
  inner_join(get_sentiments("bing")) 

news_sentiment <- news_df %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(index = line, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)


news_sentiment
```
```{r}
ggplot(news_sentiment, aes(index, sentiment)) +
  geom_col(show.legend = FALSE)+
  scale_x_discrete(limits = c(0, 2, 4, 6, 8))
```

As the result shown, our article are likely to be negative. (Again we have a very small data set)

## 2.3 Comparing the three sentiment dictionaries
Let's use all three sentiment lexicons on the article.

```{r}
afinn <- news_df %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = line) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

afinn

bing <- news_df %>% 
  inner_join(get_sentiments("bing")) %>%
  mutate(method = "bing")
bing

nrc <- news_df %>% 
  inner_join(get_sentiments("nrc")) %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  mutate(method = "NRC")
nrc

bing_and_nrc <- bind_rows(bing, nrc) %>% 
  count(method, index = line, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)
bing_and_nrc

# plotting
bind_rows(afinn, bing_and_nrc) %>% 
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col() +
  facet_wrap(~method, ncol = 1, scales = "free_y") + 
  scale_x_discrete(limits = c(0, 2, 4, 6, 8, 10, 12))
  
```

As we can see in the plot, three different lexicons for calculating sentiment in our article gives very different result due to the small data size. 

However, according to the book, we see similar dips and peaks in sentiment at about the same places in the novel, but absolute values are significantly different. AFINN lexicon gives largest absolute value, Bing gives lower absolute values, and the NRC labeled the text more positively.

"the NRC sentiment is high, the AFINN sentiment has more variance, the Bing et al. sentiment appears to find longer stretches of similar text, but all three agree roughly on the overall trends in the sentiment through a narrative arc."

Why is NRC lexicon provided higher result compared to the Bing?
```{r}
get_sentiments("nrc") %>% 
     filter(sentiment %in% c("positive", "negative")) %>% 
count(sentiment)

get_sentiments("bing") %>% 
  count(sentiment)
```

As we can see, both lexicons have more negative words than positive words, but the ratio of negative to positive words is higher in the Bing than NRC. This is the reason that we see higher sentiment scores in the NRC.

Also, if the negative words in the NRC do not match the words we are analyzing very well, then "whatever the source of these differences, we see similar relative trajectories across the narrative arc, with similar changes in slope, but marked differences in absolute sentiment from lexicon to lexicon."

This is important to keep in mind when choosing a sentiment lexicon for analysis. 

## 2.4 Most common positive and negative words
```{r}
nrc_word_count <- nrc %>% 
  count(word, sentiment, sort = TRUE)

nrc_word_count

#plot

nrc_word_count %>% 
  group_by(sentiment) %>% 
  ggplot(aes(word,n,fill = sentiment)) +
  geom_col(show.legend = FALSE) + 
   facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment", x = NULL) +
  coord_flip()
```

Problem: Since my article data set is small, we can spot problem in my analysis that patient and confirmed are negative in my case. How do we spot there kinds of problem when we have very big data set?

## 2.5 Wordclouds 
```{r}
library(janeaustenr)
library(stringr)

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",                             ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

library(reshape2)
tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

Note that since my data set is small, I am using the data set in the text book to create these wordcloud.

To tag positive and negative words, we will have to turn the dataframe into matrix with reshape2 (acast()) function and use comparison.cloud().

The size of the word's test in the wordcloud is in proportion to its frequency in the document. We use word cloud to visually see the most frequent positive and negative words, but the size is not comparable across sentiments

## 2.6 Looking at units beyond just words
Sometimes it is useful to look at different unit of text. For example, try to understand the sentiment of a sentence as a whole. We can tokenize text into sentences.
```{r}
news_sentences <- tibble(line = 1:15, text = news)

news_sentences <- news_sentences %>%
  unnest_tokens(sentence, text, token = "sentences")
news_sentences
```

We can use unnest_tokens() to split into tokens using a regex patter. It is not needed for my case.

Now, we can find the proportion of negative words in the article. 

```{r}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
word_count <- nrow(news_df)


news_df %>%
  semi_join(bingnegative)%>%
  summarize(negativewords = n()) %>%
  mutate(ratio = negativewords/word_count)

```

We can compare the negative words ratio with other documents, since it is normalized by the text length. 

## 2.7 Summary
Sentiment analysis can help understand the attitudes and opinions expressed in text. "We can use sentiment analysis to understand how a narrative arc changes throughout a book or what words with emotional and opinion content are important for a particular text."
We can also compare different emotional and opinion content between different documents. 

## 3 Analyzing word and document frequency: tf-idf
term frequency(tf) is measure of how frequently a word occurs in a document.

inverse document frequency(idf) which decrease the weight for commonly used words and increases the weight for words that are not used very often. 

These two frequency can be combined to calculate tf-idf. So the frequency of a term is adjusted for how rarely it is used

tf-idf - "intended to measure how important a word is to a document in a collection (or corpus) of documents"

The inverse document frequency for any given term is defined as:

$$idf(term) = ln(\frac{n_{documents}}{n_{documents Containg Term}}) $$

## 3.1 Term frequency in Jane Austen's novels
From previous examples, I realized that my data set was too small for our model. I used more articles as my data set to run the examples blow.

https://www.cbc.ca/news/canada/world-canada-covid-19-sept-15-1.5724311
https://www.ctvnews.ca/health/coronavirus/canada-very-well-might-be-starting-its-second-wave-of-covid-19-doctor-says-1.5105105
https://ottawacitizen.com/news/local-news/covid-19-five-deaths-in-west-end-villa-outbreak-51-new-cases-in-ottawa-elliott-says

```{r include=FALSE}
newsdata <- c("Some provinces are scrambling to increase testing capacity as coronavirus infections spike across Canada and lineups at COVID-19 testing sites see a significant influx of people. 

In order to accommodate demand, opening hours at two Ottawa assessment centres will be extended in the coming days, Ottawa Public Health, Children's Hospital of Eastern Ontario and Ottawa Hospital said in a joint statement Monday afternoon. 

The statement said the health authorities are hiring more staff and training them so that the Brewer assessment centre can accept patients for 12 hours per day, seven days a week — four more hours per day than it is normally open.

We knew that with the kids returning to school we would see these volumes. To prepare, we have tripled staffing in the last month for testing children and youth at the [Brewer Arena assessment centre]. More are being trained and still more are being hired, the statement said. As of 3:15 p.m. ET on Tuesday, Canada had 138,572 confirmed or presumptive coronavirus cases. Provinces and territories listed 121,555 of those as recovered or resolved. A CBC News tally of deaths based on provincial reports, regional health information and CBC's reporting stood at 9,226.

At a news conference Tuesday, Ontario Premier Doug Ford said the coronavirus is continuing to spread.

The province announced 251 new cases Tuesday, with the majority of those cases found in Toronto, Ottawa and Peel region with 73, 51 and 42 cases, respectively. 

As we see around the world, countries are getting hammered by COVID-19, said Ford.

He also said he believes a second wave of the virus is coming to the province and said officials are cautioning that the second wave could be more complicated than the first one. Ford said Ontario is now expanding testing and building up a supply of personal protective equipment in anticipation of a continued spike in cases. 

At an Ottawa council meeting last Wednesday, elected officials from across the city called for an expansion of its testing system to better meet demand.

Part of our future success will depend on our ability to test, to test rapidly and to remove barriers to access to testing, said Rideau-Vanier Coun. Mathieu Fleury in an interview with CBC.

In London, Ont., a long line of cars was seen waiting outside the city's only open assessment centre — the Carling Heights Optimist Centre — on Sunday.

The Middlesex-London Health Unit said the lengthy wait was partly due to a staffing shortage at that location.

Many of the cars in line at the Carling Heights Optimist Centre were filled with young people looking to get tested.Quebec recorded 291 new cases of COVID-19 on Tuesday, which is the sixth day in a row the province has reported more than 200 cases.

In response to the rising number of infections, Quebec Premier François Legault warned the public that social gatherings must be limited immediately in order to avoid closing schools and businesses.

The situation is critical. It's worrisome, and we must act now, he said at a news conference Tuesday. Legault also said Quebec faces a real risk of a second wave.

Quebec City and the Lower Saint-Lawrence regions are being closely monitored and may be moved to the orange alert level, up from yellow, indicating an increased risk of COVID-19 to the public, sources told Radio-Canada.

Health Minister Christian Dubé said at the same news conference that an orange classification would mean the closure of bars and reducing the number of people allowed in private gatherings from 10 to six. At the federal level, Chief Public Health Officer Dr. Theresa Tam said Tuesday that the government is currently working closely with provincial microbiology labs to enhance test processing capacity. 

Tam told reporters that the current national capacity is beyond 60,000 [tests per day] at the national level.  

She said Canada needs to augment the portfolio of testing capabilities in Canada to include new technology like rapid saliva tests.  

Testing issues have also been reported in St. John's, with local mother Flora Salvo saying she spent four days on the phone trying to book a COVID-19 test and that the reservation system needs to be revamped.

She said the painfully slow process of getting tested — from her first call to when she received a negative result last Saturday — stretched over a full week. 
" ,

"TORONTO -- Rising numbers of COVID-19 cases in multiple provinces are stoking fears of a potential second wave, and one infectious disease expert says this surge in infections might 'very well' be the start of that next phase in the pandemic.

Infectious disease specialist Dr. Isaac Bogoch says that current upward trends in B.C., Alberta, Manitoba, Ontario and Quebec may be fuelling Canada’s second wave of coronavirus infections.

It might be, it very well might be. We're certainly seeing these cases rumble up in the wrong direction, and quite frankly what happens over the next few weeks and then over the next month or two ahead really depends on us. If we let our guard down as citizens, if we let our guard down for example as businesses and organizations, then we'll see a spike in cases, Bogoch told CTV's Your Morning on Tuesday.
", 

"
Knoxdale-Merivale councillor and Ottawa Board of Health chair Keith Egli said Tuesday that school reopenings remain a priority, particularly in light of concerns over children’s mental health and emotional well-being during the past six months.

“All society is impacted when people cannot rely on schools to support childhood development and economic activity,” he said, adding that Ottawa Public Health has prioritized working with school boards to help ensure a successful return to classes, meeting weekly to review plans and feedback, and provide guidance.

Dr. Etches also addressed the possibility, raised earlier in the day by Ontario Premier Doug Ford, of the province’s returning to stricter measures and lockdowns to help contain the spread of COVID-19, especially in current hotspots such as Ottawa, Toronto and the Peel region.

“We’re in favour of a risk-based approach, absolutely. We know that larger centres are always going to have a greater risk of transmission of COVID, for many reasons — density of population and sheer numbers of people — so we absolutely support an approach based on risk.”

Ottawa will be expanding the hours and capacity of its care clinics and COVID-19 assessment centres, as school reopenings have led to greatly increased lineups and wait times at those sites, particular at Brewer Arena.

Dr. Alan Forster, the testing strategy lead with the Champlain COVID-19 Response Committee, made the announcement Tuesday, indicating that the levels of testing in the area, currently at about 2,000 per day, could increase to 3,000 with increased hours and staffing, and as much as 3,500 if mobile testing in schools and other such venues is added.


Forster said he expects that Brewer Arena and the Coventry Road sites will increase from eight hours a day to 12, seven days a week. Care clinics operated by the Queensway-Carleton and Montfort hospitals will expand their weekend hours.

The changes, he added, are expected to take place in the next week.

Yet Ottawa’s chief medical officer of health, Dr. Vera Etches, urged residents to only seek out testing if they display symptoms of have come in direct contact with those who have tested positive.

Instead, she recommended that people reexamine and possibly modify their behaviours to ensure they’re limiting the number of close contacts in their circle and following other precautions, including avoiding large social gatherings.

“There is potential harm when the value of asymptomatic testing is low, and it’s displacing people who need to have a test,” said Etches. “And the labs need to be able to turn around the results quickly for controlled outbreaks and school-setting investigations.”

Etches reinforced Ottawa Public Heath’s position that social networks, and not businesses, are largely driving infections up. Most of the cases, she said, are related to outbreaks, such as those at long-term care facilities, or, in about one-quarter of cases, of unknown origin.

“The rest of the cases are what we call sporadic, either usually linked to a household close contact or a social group close contact. That is the most common source of exposure.

“We have very few workplace outbreaks that have been identified,” she added, “but we know everything’s connected.

“The call today is for each of us to think about how many close contacts did you have this week where you were within two metres, especially indoors without a mask on? In each of those situations, that could be connected to a chain of transmission.

“We want everybody to do their part to decrease those numbers of close contacts in our lives.”

Ottawa reported 52 new cases of COVID-19 on Tuesday, down from 61 reported Monday.

Five residents at West End Villa long-term care home have died from complications resulting from COVID-19.

Extendicare, which manages the facility, confirmed the deaths in a statement from West End Villa administrator Kelly Keller on Monday.

One of the deaths had previously been reported.

“We are deeply saddened to share that as of September 14, five of our residents have passed away from complications related to COVID-19,” Keller wrote in statement. “We have been in contact with the families of these residents and offer them our condolences in this difficult time. All families with loved ones at Extendicare West End Villa have also been informed.

“We cannot comment further on our residents out of respect for their privacy and the privacy of their families.”

The 242-bed Elmira Avenue facility was listed as being in outbreak status by Ottawa Public Health effective Aug. 30. According to the province’s latest figures, 46 residents and nine staff members have tested positive.

Advertisement
Article content continued
“We have conducted a second round of COVID-19 surveillance testing to help ensure our cohorting efforts are as effective as possible,” added Keller, “and we expect to receive the results of those tests over the coming days.

Ottawa Public Health reported 52 new COVID-19 cases and four new deaths Tuesday, bringing those totals to 3,335 and 272, respectively.

Nine Ottawa residents are currently hospitalized with COVID-19, but none are in intensive care.

There are currently 362 active cases in Ottawa, while 2,753 have been resolved.

Additionally, OPH confirms 19 health-care or child-care establishments with outbreaks, an increase of one from Monday’s report. The most recent addition to OPH’s institutional outbreak list is Riverview Development Services, which reported one staff case.

Meanwhile, two more Ottawa French-language schools have reported people who have COVID-19, bringing the total number of people testing positive to 11 at nine Ottawa schools.

The new Ottawa schools identified in the provincial database on Tuesday are Marius-Barbeau elementary, with one case, and Gabrielle-Roy elementary, with two cases. All three individuals were not staff or students, but are listed as “other.”

Provincial

Ontario Premier Doug Ford hinted on Tuesday that the province may soon offer asymptomatic COVID-19 testing through pharmacies such as Shopper’s Drug Mart.

Citing the long lineups for testing, particularly after school openings in the province have prompted more people to seek testing, Ford indicated that he spoke earlier in the day with the CEO of Shopper’s, and that an announcement on the matter is forthcoming.

“You’ll be hearing from us over the next couple of days,” Ford said. “I just want to make sure that all the ducks are in a row and then we’ll make an announcement.

“I’m not going to say that 100 per cent, but we’re all over it.”

The premier also would not rule out the possibility of the province further clamping down on social gatherings to help stem the recent increase in COVID-19 cases, indicating that he’s been in talks with Ottawa Mayor Jim Watson, Toronto Mayor John Tory and Brampton Mayor Patrick Brown to get their input on the issue.")

newsdata
news_word <- tibble(article = 1:3, text = newsdata)
news_word
```


We want to examine the term frequency
We first tokenized our data, and check the most common used words in our data. 

```{r}
news_word <- news_word %>% 
  unnest_tokens(word, text) %>% 
  count(article, word, sort = TRUE)
news_word

total_words <- news_word %>% 
  group_by(article) %>% 
  summarize(total = sum(n))
total_words

news_word <- left_join(news_word, total_words)
news_word
```

Here in the news_word data frame, we have each word_article combination per row. n is the number of occurrence of that word in the article. Total is the total words in that article. We can look at the distribution of n/total for each article.

```{r}
ggplot(news_word, aes(n/total, fill = article)) +
  geom_histogram(show.legend = FALSE) +
   facet_wrap(~article, ncol = 2, scales = "free_y")
```

All three article have very long tails to the right(extremely rare words). These plot exhibit similar distributions for our article and the book example used in our textbook, with many words that occur rarely and fewer words occur frequently.

## 3.2 Zipf's law
The distribution shown in 3.1 are typical in language. It is so common that given any corpus of natural language the relationship between the frequency that a word appears is inversely proportional to its rank. - This is the Zipf's law.

We can examine Zipf's law using our article data.
```{r}
freq_by_rank <- news_word %>% 
  group_by(article) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)
freq_by_rank
```

The rank column represents the rank of each word in each article. 

Zipf’s law is often visualized by plotting rank on the x-axis and term frequency on the y-axis, on logarithmic scales. Plotting this way, an inversely proportional relationship will have a constant, negative slope.
```{r}

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = article)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

We see that the relationship between rank and frequency does have a negative slope
```{r}
rank_subset <- freq_by_rank %>% 
  filter(rank < 80,
         rank > 20)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = article)) +
  geom_abline(intercept = -1.1419, slope = -0.7247, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

## 3.3 The bind_tf_idf function
The idea of tf-idf is to find the important words in our document by decreasing the weight for common words and increase the weight for rare words. Calculating tf-idf to find important words but not too common. 
```{r}
news_word <- news_word %>% 
  bind_tf_idf(word, article, n)
```

Notice that idf and tf-idf are zero for extremely common words. The inverse document frequency will be higher for words that occur in fewer of the documents in the collections. In opposite, the inverse document frequency and tf-idf is very low (close to zero) for words that occur in many of the articles.

Now, let's look at a visualization for the ti-idf words in our article

```{r}
news_word <-news_word%>% 
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))
news_word
news_word %>%
  arrange(desc(tf_idf)) %>%
  group_by(article) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  ggplot(aes(word, tf_idf, fill = article)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~article, ncol = 2, scales = "free") +
  coord_flip()
```

## 3.4 A corpus of physics texts
In the book, they analyzed some classic physics from package Project Gutenberg using tf-idf. For me I will review the steps for tf-idf analysis.

* use unnest_tokens() to tokenzie each words in our data
* use count() to find how many times each word was used
* use bind_tf_idf() to calculate tf-idf (considering all text have different length)
* use ggplot() to create visualizations showing the highest tf-df words in the data
* check our result, check for names and some less meaningful words.
* remove the less meaningful words by using a custom list of stop words and use anti_join to remove them. 
* recalculate the tf-idf using bind_tf_idf() and remake the plot!


## 3.5 summary
Using tf-idf allows us to find words that are special to one document within a collection of documents and how different words are important in documents within a collection or corpus of documents.

## 4 Ralationships between words: n-grams and correlations
So far we working on word as individual units and considered their relationship to sentiments or to documents. However, we can also analyses based on the relationships between words. Examining which words tend to follow others immediately, or which words tend to co-occur within the same documents.

## 4.1 Tokenizing by n-gram
We can use unnest_tokens() to tokenize text into consecutive sequences of words, called n-grams. By seeing how often word X is followed by word Y, we can build a model of the relationship between them. We can do it by ading token = "ngrams" when using unnest_tokens(), and setting n to the number of words we want to capture in each n-gram. (when n = 2, examining pairs of two consecutive words, also called "bigrams")

```{r}
news_word <- tibble(article = 1:3, text = newsdata)
news_bigrams <- news_word %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
news_bigrams
```

Note that this data structure is still in tidy text format(one-token-per-row), but each token now represents a bigram.

## 4.1.1 Counting and filtering n-grams
We can see the most common bigrams using count()

```{r}
news_bigrams %>% 
  count(bigram, sort = TRUE)
```

A lot of most common bigrams are pairs of common words (of the, to the) these may be uninteresting. So, we can use separate() function to splot a column into multiple based on a delimiter. We can split the bigrams into "word1" and "word2", then we can remove the stop words.

```{r}
bigrams_separated <- news_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

news_bigrams %>% 
  count(bigram, sort = TRUE)
bigram_counts
```

Then, we can recombine the words into bigrams by using unite()

```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```

In some cases, we may be interested in the most common trigrams (n = 3), which are consecutive sequences of 3 words.

## 4.1.2 Analyzing bigrams
A bigram can also be treated as a term in a document. Thus, we can look at the tf-idf of the bigrams in the articles. 

```{r}
bigram_tf_idf <- bigrams_united %>%
  count(article, bigram) %>%
  bind_tf_idf(bigram, article, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf

bigram_tf_idf %>% 
  group_by(article) %>% 
  top_n(3) %>% 
  ungroup() %>% 
  ggplot(aes(bigram, tf_idf, fill = article)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~article, ncol = 2, scales = "free") +
  coord_flip()
```

The advantage to examining the tf-idf of bigrams is that it may capture structure that isn't present when counting single words, and may provide context more understandable. However, the disadvantages are that a typical two-word pair is rarer than single word. Thus, bigrams can be especially useful when we have large data set. In our case, it is not very useful to use bigram to analysis our article.

## 4.1.3 Using bigrams to provide context in sentiment analysis

Now that we have our data organized into bigrams, it's east to tell how often words are preceded by a word "not"

```{r}
bigrams_separated %>% 
  filter(word1 == "not") %>% 
  count(word1, word2, sort = TRUE)
```

By performing sentiment analysis on bigram, we can see how often sentiment-associated words are preceded by "not" or other negating words. We may use this to ignore or reverse the sentiment socre.

Let's use AFINN for sentiment analysis. 
```{r}
AFINN <- get_sentiments("afinn")

not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)

not_words

```

unfortunately, because of our small data set, there are only 6 bigrams that contains "not", and the second word following the "not" in these 6 bigrams does not have a sentiment score in AFINN data set. 

We can show which words contributed the most in the "wrong direction" in our sentiment analysis by computing the sentiment score and the number of times they appear. Then we can visualized the result with a bar plot.

Looking at the bar plot, we can see the largest causes of misidentification(words that make the text seem much more positive/negative than it is)

```{r}

negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)
negated_words

```

We could then visualize what the most common words to follow each particular negation are.
However, again our data set was too small to do it.


## 4.1.4 Visualizing a network of bigrams with ggraph
We can arrange the words into a network to visualize the relationship among words. It can be constructed from a tidy object with three variables:

* from: the node an edge is coming from
* to: the node an edge is going towards
* weight: A numeric value associated with each edge

We can use graph_from_data_frame() to produce an igraph object. It takes a data frame of edges with columns for "from", "to", and edge(in this case n)
```{r}
bigram_graph <- bigram_counts %>%
  filter(n >1) %>% 
  graph_from_data_frame()

bigram_graph
```


Then, we can convert an igraph object into a ggraph to create the graph. 
```{r fig.width =10}
set.seed(2017)
library(ggraph)
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

This graph shows the common bigrams that occurred more than once in our articles. 

Something we can do to make a better looking graph:

* add edge_alpha aesthetic to the link layer to make links transparent based on how common or rare the bigram is.

* add directional arrow by using grid::arrow(), including an end_cap option that tells the arrow to end before touching the node.

* add a theme using theme_void()
```{r fig.width =10}
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

"Note that this is a visualization of a Markov chain, a common model in text processing. In a Markov chain, each choice of word depends only on the previous word."

## 4.1.5 Visualizing bigrams in other texts
We can write a function for clearning and visualizing bigrams on any text data set.

```{r}
library(dplyr)
library(tidyr)
library(tidytext)
library(ggplot2)
library(igraph)
library(ggraph)
count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}
```

Using these count_bigrams and visualize_bigram function, we can easily perform data cleaning and visualizing bigrams on other text dataset.

## 4.2 Counting and correlating pairs of words with the widyr package
Tokenizing by n-gram is a useful way to explore pairs of adjacent words. However, we may be interested in words that tend to co-occur within particular documents, even if they are not next to each other. 
It is easy for tidy data to compare between variables or grouping by rows, but it is hard to compare between rows. For example, to count the number of times that two words appear within the same document, or to see how correlated they are. We may use widyr package manipulate the data that can compare between rows.

## Counting and correlating among sections
```{r}
news_word <- tibble(article = 1:3, text = newsdata)
news_word <- news_word %>% 
  unnest_tokens(word, text) %>% 
  filter(!word %in% stop_words$word)

news_word

library(widyr)

# count words co-occuring within sections
word_pairs <- news_word %>%
  pairwise_count(word, article, sort = TRUE)

word_pairs
```

The result from the pairwise_count will count the common pairs of words co-appearing within our the same article.

Now, we can see the words that most often occur with "Canada" in our article.
```{r}
word_pairs %>% 
  filter(item1 == "canada")
```

## 4.2.2 Pairwise correlation
We may also examine correlation among words, which indicates how often they appear together relative to how often they appear separately. 

phi coefficient - a common measure for binary correlation

$$\phi = \frac{n_{11}n_{00} - n_{10}n_{01}}{\sqrt{n_{1.}n_{0.}n_{.0}n{.1}}}$$
where 

* $n_{11}$ is the number of times both words appear in the document
* $n_{00}$ is the number of times neither words appear
* $n_{10}$ is the number of times first word appear and second word does not
* $n_{01}$ is the number of times second word appear and first word does not
* $n_{1.}$ is the total number of times first word appear
* $n_{0.}$ is the total number of times first word does not appear
* $n_{.1}$ is the total number of times second word appear
* $n_{1.}$ is the total number of times second word does not appear
* $n$ is the total number of words

The pairwise_cor() function in widyr lets us find the phi coefficient between words based on how often they appear in the same section.

```{r}
word_cors <- news_word %>%
  group_by(word) %>% 
  filter(n() >= 5) %>% 
  pairwise_cor(word, article, sort = TRUE)


word_cors %>% 
  filter(item2 == "wildfires")
```

Because our data set is too small, the correlation is not very interesting. :(

If we have bigger data set, we may use a bar plot to plot the correlation. 


## 4.3 Summary
This chapter, we found relationship and connections between words in a form of n-grams. It enable us to see what words tend to appear after others, or co-occurrences and correlations. I also learn about network visualizations to show the connections between words. 